---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: volsync-rd-refresh
  namespace: storage
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: volsync-rd-refresh
rules:
  - apiGroups: ["volsync.backube"]
    resources: ["replicationdestinations"]
    verbs: ["get", "list", "watch", "patch"]
  # Permissions for cleaning up completed VolSync jobs and pods
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["get", "list", "delete"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: volsync-rd-refresh
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: volsync-rd-refresh
subjects:
  - kind: ServiceAccount
    name: volsync-rd-refresh
    namespace: storage
---
# Cleanup CronJob to remove completed VolSync destination jobs/pods
# This prevents PVCs from getting stuck in Terminating state due to
# the kubernetes.io/pvc-protection finalizer referencing completed pods
apiVersion: batch/v1
kind: CronJob
metadata:
  name: volsync-cleanup
  namespace: storage
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 300
      backoffLimit: 1
      template:
        spec:
          serviceAccountName: volsync-rd-refresh
          restartPolicy: OnFailure
          containers:
            - name: cleanup
              image: docker.io/alpine/k8s:1.34.1
              imagePullPolicy: IfNotPresent
              command:
                - /bin/sh
                - -c
                - |
                  set -euo pipefail
                  echo "=== VolSync Cleanup $(date -Iseconds) ==="

                  # Find and delete completed volsync-dst pods older than 5 minutes
                  # These pods block PVC deletion due to kubernetes.io/pvc-protection finalizer
                  echo "Checking for completed VolSync destination pods..."
                  kubectl get pods --all-namespaces \
                    -l app.kubernetes.io/created-by=volsync \
                    --field-selector=status.phase=Succeeded \
                    -o jsonpath='{range .items[*]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' | \
                  while read -r ns name; do
                    [ -z "$ns" ] && continue
                    # Only target destination pods (volsync-dst-*)
                    case "$name" in
                      volsync-dst-*)
                        echo "Deleting completed pod: $ns/$name"
                        kubectl delete pod -n "$ns" "$name" --ignore-not-found=true
                        ;;
                    esac
                  done

                  # Also clean up completed volsync-dst jobs without TTL
                  echo "Checking for completed VolSync destination jobs..."
                  kubectl get jobs --all-namespaces \
                    -l app.kubernetes.io/created-by=volsync \
                    -o jsonpath='{range .items[?(@.status.succeeded==1)]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' | \
                  while read -r ns name; do
                    [ -z "$ns" ] && continue
                    # Only target destination jobs (volsync-dst-*)
                    case "$name" in
                      volsync-dst-*)
                        echo "Deleting completed job: $ns/$name"
                        kubectl delete job -n "$ns" "$name" --ignore-not-found=true
                        ;;
                    esac
                  done

                  echo "Cleanup complete"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: volsync-rd-refresh
  namespace: storage
spec:
  schedule: "30 * * * *"
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 3600
      backoffLimit: 1
      template:
        spec:
          serviceAccountName: volsync-rd-refresh
          restartPolicy: OnFailure
          containers:
            - name: kubectl
              image: docker.io/alpine/k8s:1.34.1
              imagePullPolicy: IfNotPresent
              command:
                - /bin/sh
                - -c
                - |
                  set -euo pipefail
                  kubectl get replicationdestinations --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' | while read ns name; do
                    case "$name" in *-manual) echo "skip $ns/$name"; continue;; esac
                    echo "patch $ns/$name"
                    kubectl -n "$ns" patch replicationdestination "$name" --type merge -p '{"spec":{"trigger":{"manual":"'"$(date +%s)"'"}}}'
                  done
