---
# Helm values for kube-prometheus-stack deployment
crds:
  enabled: true
  upgradeJob:
    enabled: true
    forceConflicts: true
cleanPrometheusOperatorObjectNames: true
# Alertmanager configuration - defined here for global routing (not namespace-scoped)
alertmanager:
  alertmanagerSpec:
    # Disable AlertmanagerConfig CRD processing - we use global config below
    alertmanagerConfigSelector:
      matchLabels:
        alertmanagerConfig: disabled
    # Use our config secret directly (prevents operator from overwriting)
    useExistingSecret: true
    configSecret: alertmanager-kube-prometheus-stack
    # Mount secrets for notification receivers
    secrets:
      - alertmanager-pushover
      - alertmanager-discord
  config:
    global:
      resolve_timeout: 5m
    route:
      receiver: 'pushover'
      group_by: ['alertname', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      routes:
        - matchers:
            - alertname = "Watchdog"
          receiver: 'null'
        - matchers:
            - alertname = "InfoInhibitor"
          receiver: 'null'
        # Send all other alerts to notification receivers
        - receiver: 'log-aggregator'
          continue: true
        - receiver: 'pushover'
          continue: true
        - receiver: 'discord'
    inhibit_rules:
      - source_matchers:
          - severity = "critical"
        target_matchers:
          - severity =~ "warning|info"
        equal: ['namespace', 'alertname']
      - source_matchers:
          - severity = "warning"
        target_matchers:
          - severity = "info"
        equal: ['namespace', 'alertname']
      - source_matchers:
          - alertname = "InfoInhibitor"
        target_matchers:
          - severity = "info"
        equal: ['namespace']
    receivers:
      - name: 'null'
      - name: 'log-aggregator'
        webhook_configs:
          - url: 'http://log-aggregator.tools.svc.cluster.local:8080/api/alert'
            send_resolved: true
      - name: 'pushover'
        pushover_configs:
          - user_key_file: /etc/alertmanager/secrets/alertmanager-pushover/pushover_user
            token_file: /etc/alertmanager/secrets/alertmanager-pushover/pushover_token
            send_resolved: true
            title: '{{ if eq .Status "resolved" }}[RESOLVED] {{ end }}{{ if .CommonAnnotations.summary }}{{ .CommonAnnotations.summary }}{{ else }}{{ .CommonLabels.alertname }}{{ end }}'
            message: '{{ if eq .Status "resolved" }}This alert has cleared. {{ else }}New alert firing. {{ end }}Severity: {{ or .CommonLabels.severity "unknown" }}. {{ if .CommonAnnotations.description }}{{ .CommonAnnotations.description }}{{ else if .CommonAnnotations.summary }}{{ .CommonAnnotations.summary }}{{ else }}Alert {{ .CommonLabels.alertname }} in {{ if .CommonLabels.namespace }}{{ .CommonLabels.namespace }}{{ else }}unknown{{ end }}{{ end }}{{ if .CommonLabels.cluster }} on cluster {{ .CommonLabels.cluster }}.{{ end }}'
      - name: 'discord'
        discord_configs:
          - webhook_url_file: /etc/alertmanager/secrets/alertmanager-discord/webhook
            send_resolved: true
            title: '{{ if eq .Status "resolved" }}[RESOLVED] {{ end }}{{ if .CommonAnnotations.summary }}{{ .CommonAnnotations.summary }}{{ else }}{{ .CommonLabels.alertname }}{{ end }}'
            message: '{{ if eq .Status "resolved" }}This alert has cleared. {{ else }}New alert firing. {{ end }}Severity: {{ or .CommonLabels.severity "unknown" }}. {{ if .CommonAnnotations.description }}{{ .CommonAnnotations.description }}{{ else if .CommonAnnotations.summary }}{{ .CommonAnnotations.summary }}{{ else }}Alert {{ .CommonLabels.alertname }} in {{ if .CommonLabels.namespace }}{{ .CommonLabels.namespace }}{{ else }}unknown{{ end }}{{ end }}{{ if .CommonLabels.cluster }} on cluster {{ .CommonLabels.cluster }}.{{ end }}'
kubeEtcd:
  service:
    selector:
      component: kube-apiserver
kubeProxy:
  enabled: false
prometheus:
  ingress:
    enabled: false
  prometheusSpec:
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    scrapeConfigSelectorNilUsesHelmValues: false
    podAntiAffinity: ""
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: cnpg.io/podRole
                    operator: In
                    values: ["primary"]
              topologyKey: kubernetes.io/hostname
    serviceMonitorSelectorNilUsesHelmValues: false
    enableAdminAPI: true
    walCompression: true
    enableFeatures:
      - memory-snapshot-on-shutdown
    evaluationInterval: 1m
    retention: 14d
    retentionSize: 32GB
    externalLabels:
      cluster: Executor
    # Increase probe timeouts to prevent CrashLoopBackoff during chunk snapshot operations
    # Using strategic merge patch to override probe configuration
    containers:
      - name: prometheus
        livenessProbe:
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 6  # Allow 60s of failures before restart
        readinessProbe:
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 3  # Allow 30s of failures before marking unready
        startupProbe:
          timeoutSeconds: 10
          periodSeconds: 15
          failureThreshold: 60  # Allow 15 minutes for startup
    resources:
      requests:
        cpu: 500m
      limits:
        memory: 4Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-rbd
          resources:
            requests:
              storage: 50Gi
prometheus-node-exporter:
  fullnameOverride: node-exporter
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels: ["__meta_kubernetes_pod_node_name"]
          targetLabel: kubernetes_node
kube-state-metrics:
  fullnameOverride: kube-state-metrics
  metricLabelsAllowlist:
    - pods=[*]
    - deployments=[*]
    - persistentvolumeclaims=[*]
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels: ["__meta_kubernetes_pod_node_name"]
          targetLabel: kubernetes_node
grafana:
  enabled: false
  forceDeployDashboards: true
additionalPrometheusRulesMap:
  dockerhub-rules:
    groups:
      - name: dockerhub
        rules:
          - alert: DockerhubRateLimitRisk
            annotations:
              summary: Kubernetes cluster Dockerhub rate limit risk
            expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
            labels:
              severity: critical
  oom-rules:
    groups:
      - name: oom
        rules:
          - alert: OomKilled
            annotations:
              summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            labels:
              severity: critical
  zfs-rules:
    groups:
      - name: zfs
        rules:
          - alert: ZfsUnexpectedPoolState
            annotations:
              summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
            expr: node_zfs_zpool_state{state!="online"} > 0
            labels:
              severity: critical
